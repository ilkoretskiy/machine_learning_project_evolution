{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models\n",
    "!pip install albumentations==0.4.5\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import glob\n",
    "\n",
    "import skimage.transform as sk_transform\n",
    "import skimage.filters as sk_filters\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aug(aug, min_area=0., min_visibility=0.):\n",
    "    bbox_params = A.BboxParams(format='coco', min_area=min_area, min_visibility=min_visibility, label_fields=['category_id'])\n",
    "    return A.Compose(aug, bbox_params)\n",
    "\n",
    "\n",
    "class DataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 batch_size, \n",
    "                 shuffle=True, \n",
    "                 output_size=(512,512), \n",
    "                 is_validation=False,\n",
    "                 **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self._len = len(self.dataset)\n",
    "        self.indices = range(self._len)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self._output_size = output_size\n",
    "        self.aug = self.init_aug(is_validation)        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def init_aug(self, is_validation):\n",
    "        if is_validation:\n",
    "            aug = get_aug([                      \n",
    "              A.Resize(width=self._output_size[0], height=self._output_size[1], always_apply=True),\n",
    "              A.Normalize(),\n",
    "            ], min_visibility=0.1)\n",
    "        else:\n",
    "            aug = get_aug([\n",
    "                A.RGBShift(p=0.1),\n",
    "                #A.JpegCompression(p=0.2, quality_lower=80),\n",
    "                A.OneOf([\n",
    "                  A.RandomBrightnessContrast(p=0.5),            \n",
    "                  A.HueSaturationValue(),\n",
    "                  A.RandomGamma(p=0.25),\n",
    "                  A.RandomBrightness(p=0.25),\n",
    "                  A.Blur(blur_limit=2,p=0.25),\n",
    "                ],p=0.0),\n",
    "\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.05),\n",
    "\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.05, rotate_limit=15,  border_mode=0,  p=0.2, value=(144.75479165, 137.70713403, 129.666091), mask_value=0.0 ),\n",
    "\n",
    "                #A.RandomSizedBBoxSafeCrop(width=self._output_size[0], height=self._output_size[1], erosion_rate=0.2, always_apply=True),\n",
    "                A.Resize(width=self._output_size[0], height=self._output_size[1], always_apply=True),\n",
    "              \n",
    "                A.Normalize(),\n",
    "          ], min_visibility=0.1)\n",
    "        return aug\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self._len // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Generate one batch of data. \"\"\"\n",
    "        s = index * self.batch_size % self._len\n",
    "        e = s + self.batch_size\n",
    "        indices = self.indices[s:e]\n",
    "\n",
    "        return self.__data_generator(indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Updates indices after each epoch. \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indices = np.random.permutation(self._len)\n",
    "            \n",
    "    def augment(self, img, mask):\n",
    "        label_image = label(mask)\n",
    "        bboxes = []\n",
    "        for region in regionprops(label_image):\n",
    "            if region.area >= 100:\n",
    "                minr, minc, maxr, maxc = region.bbox\n",
    "                bboxes.append((minc, minr, maxc-minc, maxr-minr ))\n",
    "                \n",
    "                \n",
    "        if len(bboxes) == 0:\n",
    "            #print (\"no bboxes\")\n",
    "            bboxes = [ [0, 0, img.shape[1], img.shape[0]] ]\n",
    "            \n",
    "\n",
    "        new_img = None\n",
    "        new_mask = None\n",
    "        try:\n",
    "            annotations = {'image': img, \n",
    "                   \"masks\" : [mask],\n",
    "                   'bboxes': bboxes,\n",
    "                   #'cropping_bbox': [minc, minr,  maxc - minc , maxr - minr],\n",
    "                   #'cropping_bbox': [0.1, 0.1, 0.2, 0.2],\n",
    "                   'category_id' : [255] * len(bboxes)}\n",
    "            \n",
    "            augmented = self.aug(**annotations)\n",
    "            new_img = augmented['image']\n",
    "            new_mask = augmented[\"masks\"][0]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_img = img\n",
    "            new_mask = mask\n",
    "        return new_img, new_mask\n",
    "        \n",
    "\n",
    "    def __data_generator(self, indices):\n",
    "        # Init the matrix\n",
    "        batch_images, batch_target = [], []\n",
    "        for idx in indices:\n",
    "            image_path, label_path = self.dataset[idx]\n",
    "            image = np.array(Image.open(image_path).convert('RGB'))\n",
    "            target = np.array(Image.open(label_path))\n",
    "\n",
    "            ## Rescale masks from [0; 255] to [0; 1]\n",
    "            target[target > 0] = 1\n",
    "            target = target.astype('float32')                                        \n",
    "            \n",
    "\n",
    "\n",
    "            image, target = self.augment(image, target)            \n",
    "            # For some unclear reasons sometimes albumentations tries to generate a crop larger then the image by itself\n",
    "            # and i didn't find any way how to catch this situtation\n",
    "            # In this case we will just resize an input image to the destination size and that's all            \n",
    "            image_shape = image.shape[:2]\n",
    "#             if image_shape[0] != self._output_size[0] or image_shape[1] != self._output_size[1]:\n",
    "#                 new_img_shape = list(image.shape)\n",
    "#                 new_img_shape[:2] = self._output_size[:2]\n",
    "#                 image = sk_transform.resize(image, output_shape=tuple(new_img_shape), preserve_range=True)\n",
    "#                 image = (image - np.array(mean)) / (np.array(std) + 1e-7)\n",
    "\n",
    "                \n",
    "#                 new_mask_shape = list(target.shape)\n",
    "#                 new_mask_shape[:2] = self._output_size[:2]                \n",
    "#                 target = sk_transform.resize(target, order=0, output_shape=tuple(new_mask_shape), preserve_range=True)\n",
    "    \n",
    "\n",
    "            # if shape of mask is not h*w*c\n",
    "            if len(target.shape) != 3:\n",
    "                ## the keras model require h*w*1\n",
    "                target = np.expand_dims(target, axis=-1)\n",
    "            \n",
    "            batch_images.append(image)\n",
    "            batch_target.append(target)\n",
    "        \n",
    "        if len(batch_images) < self.batch_size:\n",
    "            pad_images = [np.zeros_like(batch_images[0]) \n",
    "                          for _ in range(self.batch_size-len(batch_images))]\n",
    "            pad_target = [np.zeros_like(batch_target[0]) \n",
    "                          for _ in range(self.batch_size-len(batch_target))]\n",
    "            batch_images.extend(pad_images)\n",
    "            batch_target.extend(pad_target)\n",
    "\n",
    "        return np.stack(batch_images), np.stack(batch_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/kaggle/input/supervisely/processed\"\n",
    "ims_dir = os.path.join(base_dir, \"imgs\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "img_files = sorted(glob.glob(ims_dir + \"/*.*\"))\n",
    "mask_files = sorted(glob.glob(labels_dir + \"/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(img_files, mask_files))\n",
    "train_dataset, test_dataset = model_selection.train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "len(train_dataset), len(test_dataset)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, output_size=(256, 256), shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=16, is_validation=True, output_size=(256, 256), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3], include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False\n",
    "# down_stack.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "  x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.load_weights(\"/kaggle/input/segmentation/segmentation/models/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "batch = test_data_loader[0]\n",
    "image_batch = batch[0]\n",
    "mask_batch = batch[1]\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "res = model.predict(image_batch, batch_size=len(image_batch))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "  mean=(0.485, 0.456, 0.406)\n",
    "  std=(0.229, 0.224, 0.225)\n",
    "  # img = test_data_loader[batch_idx][0][img_idx]\n",
    "  reverse = ((img * std + mean) * 255).astype('uint8')\n",
    "  return reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(8, 8))\n",
    "  axes[0].imshow(res[i].squeeze())\n",
    "  axes[1].imshow(denormalize(image_batch[i]))\n",
    "  axes[2].imshow(mask_batch[i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(test_data_loader,  steps=len(test_data_loader), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = sm.losses.DiceLoss()\n",
    "binary_focal_loss = sm.losses.BinaryFocalLoss()\n",
    "total_loss = sm.losses.DiceLoss() + (1 * sm.losses.BinaryFocalLoss())\n",
    "losses = {\n",
    "          \"dice\": dice_loss,\n",
    "          \"bin focal\" : binary_focal_loss,\n",
    "          \"total\" : total_loss \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "res_copy =  res[0].squeeze().copy()\n",
    "thr = 0.2\n",
    "res_copy[res_copy <= thr] = 0.0\n",
    "res_copy[res_copy > thr] = 1.0\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(res_copy)\n",
    "\"\"\"\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for idx in tqdm.tqdm(range(len(res))):\n",
    "  MASKS_IDX = 1\n",
    "  batch_idx = idx // batch_size\n",
    "  img_idx = idx - batch_idx * batch_size\n",
    "\n",
    "  gt = test_data_loader[batch_idx][MASKS_IDX][img_idx]  \n",
    "  pred = res[idx]\n",
    "\n",
    "  img_losses = []\n",
    "  for loss_name, loss_func in losses.items():\n",
    "    loss_res = loss_func(gt, pred)\n",
    "    # loss_res = val(test_data_loader[0][1][0], np.expand_dims(res_copy, -1))\n",
    "    # print(\"{} : {}\".format(key, loss_res))\n",
    "    img_losses.append(loss_res)\n",
    "  all_losses.append(img_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_losses = map(lambda row: [float(row[0]), float(row[1]), float(row[2])], all_losses)\n",
    "new_all_losses = list(new_all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_data = pd.DataFrame(new_all_losses, columns = [\"dice\", \"focal\", \"total\"])\n",
    "losses_data.to_csv(\"/kaggle/working/losses.csv\")\n",
    "losses_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_data.hist(bins=100, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_losses = losses_data.sort_values(by=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for img_idx in np.linspace(start=0, stop=len(sorted_losses), num=10, endpoint=False):\n",
    "  item =sorted_losses.iloc[int(img_idx)]\n",
    "  print(item.name, item.total)\n",
    "  names.append(sorted_losses.iloc[int(img_idx)].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "  mean=(0.485, 0.456, 0.406)\n",
    "  std=(0.229, 0.224, 0.225)\n",
    "  # img = test_data_loader[batch_idx][0][img_idx]\n",
    "  reverse = ((img * std + mean) * 255).astype('uint8')\n",
    "  return reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in names:\n",
    "  fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "  batch_idx = idx // batch_size\n",
    "  img_idx = idx - batch_idx * batch_size\n",
    "  \n",
    "  reverse = denormalize(test_data_loader[batch_idx][0][img_idx])\n",
    "  axes[0].imshow(reverse.squeeze())\n",
    "  axes[1].imshow(test_data_loader[batch_idx][1][img_idx].squeeze())\n",
    "  axes[2].imshow(res[idx].squeeze())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "idx = sorted_losses.iloc[-1].name\n",
    "batch_idx = idx // batch_size\n",
    "img_idx = idx - batch_idx * batch_size\n",
    "\n",
    "\n",
    "axes[0].imshow(res[idx].squeeze())  \n",
    "# axes[1].imshow(test_data_loader[-1][1][-2].squeeze())\n",
    "axes[1].imshow(test_data_loader[batch_idx][1][img_idx].squeeze())\n",
    "# reverse = denormalize(test_data_loader[-1][0][-2])\n",
    "reverse = denormalize(test_data_loader[batch_idx][0][img_idx])\n",
    "axes[2].imshow(reverse.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
